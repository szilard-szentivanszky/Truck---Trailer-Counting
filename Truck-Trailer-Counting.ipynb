{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88f5a8f",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6701e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas numpy torch torchvision yolo opencv-python ultralytics PIL glob2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928fbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import numpy\n",
    "import cv2\n",
    "from torchsummary import summary\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62bc87f",
   "metadata": {},
   "source": [
    "# Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device=[\"cuda\" if torch.cuda.is_available() else \"cpu\"] # gpu switch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade27d76",
   "metadata": {},
   "source": [
    "# Data spliting --> train & validation & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset(input_dir, output_dir, train_ratio=0.7, val_ratio=0.2):\n",
    "    \n",
    "    #Split the dataset into train, validation, and test subsets.\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for subset in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_dir, subset,'' ), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, subset,'' ), exist_ok=True)\n",
    "\n",
    "    image_files = [f for f in os.listdir(input_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    total = len(image_files)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = train_end + int(total * val_ratio)\n",
    "    \n",
    "    subsets = {\n",
    "        'train': image_files[:train_end],\n",
    "        'val': image_files[train_end:val_end],\n",
    "        'test': image_files[val_end:]\n",
    "    }\n",
    "    \n",
    "    for subset, files in subsets.items():\n",
    "        for image_file in files:\n",
    "            base_name = os.path.splitext(image_file)[0]\n",
    "            annotation_file = base_name + '.txt'\n",
    "            \n",
    "            # Copy images\n",
    "            shutil.copy(os.path.join(input_dir, image_file), os.path.join(output_dir, subset, '', image_file))\n",
    "            # Copy annotations\n",
    "            if os.path.exists(os.path.join(input_dir, annotation_file)):\n",
    "                shutil.copy(os.path.join(input_dir, annotation_file), os.path.join(output_dir, subset, '', annotation_file))\n",
    "\n",
    "def augment_images_and_annotations(input_images_path, input_annotations_path, output_path, augmentations_count):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    for image_name in os.listdir(input_images_path):\n",
    "        if not image_name.endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "        \n",
    "        image_path = os.path.join(input_images_path, image_name)\n",
    "        annotation_path = os.path.join(input_annotations_path, image_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        img_height, img_width = image.shape[:2]\n",
    "        \n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"Annotation file for {image_name} not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        bboxes = parse_yolo_annotations(annotation_path, img_width, img_height)\n",
    "        \n",
    "        for i in range(augmentations_count):\n",
    "            augmented_image, augmented_bboxes = random_scaling(image, bboxes)\n",
    "            augmented_image, augmented_bboxes = random_horizontal_flip(augmented_image, augmented_bboxes)\n",
    "            augmented_image, augmented_bboxes = random_rotation(augmented_image, augmented_bboxes)\n",
    "            augmented_image, augmented_bboxes = random_shearing(augmented_image, augmented_bboxes)\n",
    "            augmented_image = adjust_brightness(augmented_image)\n",
    "            \n",
    "            # Save augmented image\n",
    "            augmented_image_name = f\"{os.path.splitext(image_name)[0]}_aug_{i}.jpg\"\n",
    "            augmented_image_path = os.path.join(output_path, augmented_image_name)\n",
    "            cv2.imwrite(augmented_image_path, augmented_image)\n",
    "            \n",
    "            # Save augmented annotations\n",
    "            augmented_annotation_path = os.path.join(output_path, augmented_image_name.replace('.jpg', '.txt'))\n",
    "            yolo_bboxes = convert_to_yolo_format(augmented_bboxes, img_width, img_height)\n",
    "            with open(augmented_annotation_path, 'w') as annotation_file:\n",
    "                annotation_file.write(\"\\n\".join(yolo_bboxes))\n",
    "\n",
    "# Example usage\n",
    "# Step 1: Split the dataset\n",
    "split_dataset(\n",
    "    input_dir=\"dataset\",\n",
    "    output_dir=\"dataset\",\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2  # Remaining 0.1 will be for the test set\n",
    ")\n",
    "\n",
    "print(\"Data split successfully completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3d003",
   "metadata": {},
   "source": [
    "# Train data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define augmentation functions\n",
    "def random_scaling(image, bboxes):\n",
    "    scale = random.uniform(0.8, 1.2)\n",
    "    height, width = image.shape[:2]\n",
    "    new_width, new_height = int(width * scale), int(height * scale)\n",
    "    scaled_image = cv2.resize(image, (new_width, new_height))\n",
    "    \n",
    "    # Adjust bounding boxes\n",
    "    new_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        class_id, x_min, y_min, x_max, y_max = bbox\n",
    "        x_min *= scale\n",
    "        y_min *= scale\n",
    "        x_max *= scale\n",
    "        y_max *= scale\n",
    "        new_bboxes.append([class_id, x_min, y_min, x_max, y_max])\n",
    "    \n",
    "    # Crop or pad to original size\n",
    "    if scale > 1.0:\n",
    "        start_x = (new_width - width) // 2\n",
    "        start_y = (new_height - height) // 2\n",
    "        cropped_image = scaled_image[start_y:start_y + height, start_x:start_x + width]\n",
    "        for bbox in new_bboxes:\n",
    "            bbox[1] -= start_x\n",
    "            bbox[2] -= start_y\n",
    "            bbox[3] -= start_x\n",
    "            bbox[4] -= start_y\n",
    "    else:\n",
    "        padded_image = np.zeros_like(image)\n",
    "        start_x = (width - new_width) // 2\n",
    "        start_y = (height - new_height) // 2\n",
    "        padded_image[start_y:start_y + new_height, start_x:start_x + new_width] = scaled_image\n",
    "        cropped_image = padded_image\n",
    "        for bbox in new_bboxes:\n",
    "            bbox[1] += start_x\n",
    "            bbox[2] += start_y\n",
    "            bbox[3] += start_x\n",
    "            bbox[4] += start_y\n",
    "    \n",
    "    return cropped_image, new_bboxes\n",
    "\n",
    "def random_horizontal_flip(image, bboxes):\n",
    "    if random.random() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "        width = image.shape[1]\n",
    "        new_bboxes = []\n",
    "        for bbox in bboxes:\n",
    "            class_id, x_min, y_min, x_max, y_max = bbox\n",
    "            new_bboxes.append([class_id, width - x_max, y_min, width - x_min, y_max])\n",
    "        return image, new_bboxes\n",
    "    return image, bboxes\n",
    "\n",
    "def parse_yolo_annotations(annotation_path, img_width, img_height):\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        annotations = []\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, bbox_width, bbox_height = map(float, parts[1:])\n",
    "            x_min = (x_center - bbox_width / 2) * img_width\n",
    "            y_min = (y_center - bbox_height / 2) * img_height\n",
    "            x_max = (x_center + bbox_width / 2) * img_width\n",
    "            y_max = (y_center + bbox_height / 2) * img_height\n",
    "            annotations.append([class_id, x_min, y_min, x_max, y_max])\n",
    "        return annotations\n",
    "\n",
    "def convert_to_yolo_format(bboxes, img_width, img_height):\n",
    "    yolo_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        class_id, x_min, y_min, x_max, y_max = bbox\n",
    "        x_center = (x_min + x_max) / 2 / img_width\n",
    "        y_center = (y_min + y_max) / 2 / img_height\n",
    "        bbox_width = (x_max - x_min) / img_width\n",
    "        bbox_height = (y_max - y_min) / img_height\n",
    "        yolo_bboxes.append(f\"{class_id} {x_center} {y_center} {bbox_width} {bbox_height}\")\n",
    "    return yolo_bboxes\n",
    "\n",
    "def augment_images_and_annotations(input_images_path, input_annotations_path, output_path, augmentations_count):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    for image_name in os.listdir(input_images_path):\n",
    "        if not image_name.endswith(('.jpg', '.png')):\n",
    "            continue\n",
    "        \n",
    "        image_path = os.path.join(input_images_path, image_name)\n",
    "        annotation_path = os.path.join(input_annotations_path, image_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        img_height, img_width = image.shape[:2]\n",
    "        \n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"Annotation file for {image_name} not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        bboxes = parse_yolo_annotations(annotation_path, img_width, img_height)\n",
    "        \n",
    "        for i in range(augmentations_count):\n",
    "            augmented_image, augmented_bboxes = random_scaling(image, bboxes)\n",
    "            augmented_image, augmented_bboxes = random_horizontal_flip(augmented_image, augmented_bboxes)\n",
    "            \n",
    "            # Save augmented image\n",
    "            augmented_image_name = f\"{os.path.splitext(image_name)[0]}_aug_{i}.jpg\"\n",
    "            augmented_image_path = os.path.join(output_path, augmented_image_name)\n",
    "            cv2.imwrite(augmented_image_path, augmented_image)\n",
    "            \n",
    "            # Save augmented annotations\n",
    "            augmented_annotation_path = os.path.join(output_path, augmented_image_name.replace('.jpg', '.txt'))\n",
    "            yolo_bboxes = convert_to_yolo_format(augmented_bboxes, img_width, img_height)\n",
    "            with open(augmented_annotation_path, 'w') as annotation_file:\n",
    "                annotation_file.write(\"\\n\".join(yolo_bboxes))\n",
    "\n",
    "# Example usage\n",
    "augment_images_and_annotations(\n",
    "    input_images_path=\"dataset/train\",\n",
    "    input_annotations_path=\"dataset/train\",\n",
    "    output_path=\"dataset/train\",\n",
    "    augmentations_count=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d326c",
   "metadata": {},
   "source": [
    "# Create a YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the images directory\n",
    "images_dir = '/dataset'\n",
    "\n",
    "# Path to the classes file\n",
    "classes_file = 'classes.txt'\n",
    "\n",
    "# Load classes from the classes file\n",
    "with open(classes_file, 'r') as f:\n",
    "    classes = [line.strip() for line in f]\n",
    "\n",
    "# Create a YAML file\n",
    "with open('data.yaml', 'w') as f:\n",
    "    f.write(f'train: {images_dir+\"/train\"}\\n')\n",
    "    f.write(f'val: {images_dir+\"/val\"}\\n')  # Same directory used for validation (can be modified)\n",
    "    f.write(f'nc: {len(classes)}\\n')\n",
    "    f.write(f'names: {classes}\\n')\n",
    "\n",
    "# Load the YAML file using PyYAML\n",
    "import yaml\n",
    "with open('data.yaml', 'r') as file:\n",
    "   prime_service = yaml.safe_load(file)\n",
    "prime_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe76318",
   "metadata": {},
   "source": [
    "# Import YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ad3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a329ff",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e221519",
   "metadata": {},
   "outputs": [],
   "source": [
    "training=model.train(\n",
    "   data='data.yaml',\n",
    "   epochs=199,\n",
    "   batch=37,\n",
    "   plots=True,\n",
    "    imgsz=640,\n",
    "   name='YOLO_11x_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289e3c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred=model.predict(\"dataset/test\", save=True, imgsz=640, conf=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd171a30",
   "metadata": {},
   "source": [
    "# Pictures prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(\"tr3.jpg\") \n",
    "for result in pred:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "      # save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab2356",
   "metadata": {},
   "source": [
    "# Implement video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "import cv2\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read a frame qqrom the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Perform object detection on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow('YOLOv11 Detection', annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a45abc",
   "metadata": {},
   "source": [
    "# Horizontal line counting -accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4baca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Horizontal\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the trained YOLOv11 model\n",
    "model = YOLO('best.pt')  # Replace with your model path\n",
    "\n",
    "# Initialize video capture\n",
    "video_path = 'video.mp4'  # Replace with your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Define ROI line position\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "roi_line_position = int(frame_width * 0.2)  # Adjust as needed\n",
    "\n",
    "# Initialize variables\n",
    "truck_count = 0\n",
    "counted_ids = set()\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform detection and tracking\n",
    "    results = model.track(frame, tracker='bytetrack.yaml')\n",
    "\n",
    "    # Process results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            class_id = int(box.cls[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            track_id = int(box.id[0]) if box.id is not None else None\n",
    "\n",
    "            # Filter for trucks\n",
    "            if class_id == 0 and confidence > 0.5:\n",
    "                # Draw bounding box and track ID\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                # Calculate centroid\n",
    "                cx = int((x1 + x2) / 2)\n",
    "                cy = int((y1 + y2) / 2)\n",
    "\n",
    "                # Check if the truck has crossed the ROI line\n",
    "                if track_id not in counted_ids and cy > roi_line_position:\n",
    "                    truck_count += 1\n",
    "                    counted_ids.add(track_id)\n",
    "                    print(f\"Truck count: {truck_count}\")\n",
    "\n",
    "    # Draw ROI line\n",
    "    cv2.line(frame, (0, roi_line_position), (frame_width, roi_line_position),\n",
    "             (255, 0, 0), 2)\n",
    "\n",
    "    # Display the truck count\n",
    "    cv2.putText(frame, f\"Truck Count: {truck_count}\", (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Advanced Truck Counter', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c5ce4",
   "metadata": {},
   "source": [
    "# Vertical line counting -accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefdb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vertical\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the trained YOLOv11 model\n",
    "model = YOLO('best.pt')  # Replace with your model path\n",
    "\n",
    "# Initialize video capture\n",
    "video_path = 'video.mp4'  # Replace with your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Define ROI line position (vertical line)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "roi_line_position = int(frame_width * 0.2)  # Adjust as needed\n",
    "\n",
    "# Initialize variables\n",
    "truck_count = 0\n",
    "counted_ids = set()\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform detection and tracking\n",
    "    results = model.track(frame, tracker='bytetrack.yaml')\n",
    "\n",
    "    # Process results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            class_id = int(box.cls[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            track_id = int(box.id[0]) if box.id is not None else None\n",
    "\n",
    "            # Filter for trucks\n",
    "            if class_id == 0 and confidence > 0.5:\n",
    "                # Draw bounding box and track ID\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'ID: {track_id}', (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                # Calculate centroid\n",
    "                cx = int((x1 + x2) / 2)\n",
    "                cy = int((y1 + y2) / 2)\n",
    "\n",
    "                # Check if the truck has crossed the ROI line\n",
    "                if track_id not in counted_ids and cx > roi_line_position:\n",
    "                    truck_count += 1\n",
    "                    counted_ids.add(track_id)\n",
    "                    print(f\"Truck count: {truck_count}\")\n",
    "\n",
    "    # Draw ROI line (vertical line)\n",
    "    cv2.line(frame, (roi_line_position, 0), (roi_line_position, frame_height),\n",
    "             (255, 0, 0), 2)\n",
    "\n",
    "    # Display the truck count\n",
    "    cv2.putText(frame, f\"Truck Count: {truck_count}\", (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Advanced Truck Counter', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66d7e2",
   "metadata": {},
   "source": [
    "# Momentary counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object counter\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('best.pt')  # Pre-trained YOLOv11 model (or replace with your own model)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "# Store the total object count\n",
    "total_objects_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Perform object detection on the current frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Get the number of objects detected in the current frame\n",
    "        current_objects_count = len(results[0].boxes)\n",
    "\n",
    "        # Update the total object count to the current count\n",
    "        total_objects_count = current_objects_count\n",
    "\n",
    "        # Generate an annotated frame with detected objects\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the total object count on the frame\n",
    "        cv2.putText(\n",
    "            annotated_frame,\n",
    "            f\"Total Objects: {total_objects_count}\",\n",
    "            (10, 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 255, 0),\n",
    "            2,)\n",
    "\n",
    "        # Display the frame in a window\n",
    "        cv2.imshow('YOLOv11 Detection', annotated_frame)\n",
    "\n",
    "        # Exit the loop if the 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        # Exit the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video resource and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
